# 【BI思考题】数据分析思维

### 1. 新零售中“人，货，场”分别指什么

”人货场“是新商业中的三大商业要素，其中”人“指的主要是消费者，而在新零售中人已经从消费者上升到了用户，而用户和消费者的区别在于用户是技术产品服务的使用者，更关注使用；而消费者是有偿获得某种产品和服务的人，侧重购买和消费。所以从这方面来说，新零售关注的人需要考虑留存性和忠诚度等因素，所以需要建立用户画像，对不同用户表现出来的特征进行分析，从而有针对性的把用户进行分层聚类，根据业务去挖掘不同种类的用户兴趣和表现，从而有目的性地去进行服务升级和投放广告。

”货“指代的是产品，而新零售针对用户需求的多元化和个性化，对于产品的认知也在升级。现在不单单只是产品，而是说为了提升产品的使用体验。而从产品定价，搭配，仓储配送，供应链管理，以及对于产品在市场上的表现（购买率，点击率等）从而针对性的对产品进行创新，升级换代等等。

”场“则是”人“和”货“所处的环境和链接，在不同的”场“中，人与货之间的联系也是不同的。场或者场景主要得作用是提供一个场与人的交互环境，在特定的场景下，人与货之间的关系可以拉近，从而提升用户的体验和使用商品的冲动。或者将货物注入影响力，风格，升级为社群等等。场所需要关注的是是场景的实施地，比如城市，商圈，地址，以及跟随时间变换场景也会有变化，比如工作日和周末的商圈情况，还比如线上和线下等等。

### 2. AIPL与传统品牌资产评估有何区别

AIPL是阿里推出的一个关于用户增长的模型，其代表了用户对于一个品牌从初识到忠诚的全过程，分为Awareness（认知），Interest（兴趣），Purchase（购买）, Loyalty（忠诚）四个阶段。

- Awareness：品牌认知人群。表示用户对品牌刚刚建立认知，了解到有这个品牌、是干啥用的，是消费者相对被动与品牌建立的接触。
- Interest：品牌兴趣人群。表示用户对这个品牌在有一定兴趣，存在着一定的购买的想法，是消费者主动与品牌建立的接触。
- Purchase：品牌购买人群。表示用户已经购买过该品牌，使用过了该品牌的产品或服务。
- Loyalty：品牌忠诚人群。表示用户对该品牌非常认同，愿意多次购买，或者愿意将该品牌推荐给自己的亲朋好友。

同样针对如上四个阶段，AIPL的计算逻辑是要根据品牌商品的曝光，点击，浏览，用户的搜索，成交，加购，分享等等行为来建立的。其使用了一个概念叫：触点。意思是品牌与消费者建立联系的一个环节或者实物。比如消费者通过电视广告了解了小米手机，电视广告就是一个触点，消费者通过天猫购买了小米手机，天猫就是一个触点。而通过触点可以收集一些有趣的数据，通过设置不一样的规则，阿里的AIPL计算逻辑如下

![](https://gitee.com/zhanghang23/picture_bed/raw/master/data%20science/1.jpg)

![](https://gitee.com/zhanghang23/picture_bed/raw/master/data%20science/2.jpg)

而AIPL与传统品牌资产评估的区别我觉得在于，阿里提出这一套模型是建立在线上模式的，可以看到其收集的数据是线上用户行为，并进行相应的量化用以来计算品牌价值。而传统品牌资产这块，难以收集消费者的行为习惯。市场调查品牌影响力和消费者音响等等都有很明显的滞后性，并且难以对这些行为习惯进行量化，从而进行深入分析，构建用户画像以及产品画像，从而针对性的去做营销和品牌升级。

### 3. 请列举一例生活工作中存在的帕累托法则

帕累托法则用来分析主要矛盾，他发现原因和结果，投入和产出，努力和报酬之间存在着无法解释的不平衡，一件事其实需要花费20%的时间完成整件事的80%，而剩下的20%都要用80%的时间来做。

美国收入前1%的人掌握了整个社会财富的95%，剩下的99%的人才分享社会财富的5%。这个就是很明显的帕累托法测实例。

### 4. 请简述GBDT与XGboost的区别

GBDT是依托于梯度提升树发展而来的决策树，而XGboost是为了高效使用GBDT的工程化实现形式。

不同点：

- 相比于GBDT只使用一阶导数，XGboost使用了二阶泰勒展开能更接近结果
- XGboost用了正则项来控制复杂度防止过拟合。
- XGboost可以处理缺失值。
- XGboost也用了跟随机森林一样的特征抽样和样本抽样的方法来防止过拟合。
- XGboost可以在分裂叶子节点得时候，计算gain执行并行计算。
- 为了提升分裂叶子节点的计算速度，首先对特征进行分桶和预排列。

### 5. 如何处理神经网络中的过拟合问题

- 通过添加正则项。
- Dropout：通过在训练过程中让部分神经元失活。
- Batch Normalization，对中间层的数据进行归一化。
- 早停法
- 数据增强